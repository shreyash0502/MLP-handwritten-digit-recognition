{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "mlpcode.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2X8-qvoBmUL"
      },
      "source": [
        "# **Multi-Layer Perceptron Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKexKe6Z_gjO"
      },
      "source": [
        "**Installing dependencies in our notebook**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dL-AR6GmtA-4",
        "outputId": "6844ebb1-41d1-4cc8-9667-2415d4702a59"
      },
      "source": [
        "!pip3 install torch numpy matplotlib "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4weOFU0_pLP"
      },
      "source": [
        "**Importing the required dependencies.**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LCg1zzstzn6"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hR1fQzrR0Vs5"
      },
      "source": [
        "**Device configuration**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFgRQlH7gCYx",
        "outputId": "0ba7a24e-2d22-45b0-a96c-ef33cb8a9559"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYq01zls_xuc"
      },
      "source": [
        "**Function to Read and format the data as list of tuples containing feature tensor and label.**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjslMA1TfoEb"
      },
      "source": [
        "def read_data(file):\n",
        "  data = []\n",
        "  f = open(file)\n",
        "  while(True):\n",
        "    s = f.readline()\n",
        "    if len(s) == 0:\n",
        "      break\n",
        "    curr = list(s.split(','))\n",
        "    data.append((torch.tensor(list(map(float, curr[:-1]))), int(curr[-1])))\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvBxvXl2zxKT"
      },
      "source": [
        "**Loading the dataset.**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgisbjvMyFai"
      },
      "source": [
        "# Edit with the path of train and test dataset\n",
        "train_dataset = read_data('optdigits.tra')\n",
        "test_dataset = read_data('optdigits.tes')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-s7lJognA9s1"
      },
      "source": [
        "**Function to train the model**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-awJ-_2A5NS"
      },
      "source": [
        "def train(model, train_dataset, criterion=nn.CrossEntropyLoss(), learn_rate=0.001, max_iter=20, batch_size=5, loss_tol=1e-3, get_stats=True):\n",
        "  from torch import optim\n",
        "\n",
        "  # making batches of training data\n",
        "  batch_size = min(batch_size, len(train_dataset))\n",
        "  train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                            batch_size=batch_size, \n",
        "                                            shuffle=True)\n",
        "\n",
        "  # using SGD optimizer\n",
        "  optimizer = optim.SGD(model.parameters(), lr=learn_rate)\n",
        "\n",
        "  n_total_steps = len(train_loader)\n",
        "  prev_loss = 1e5\n",
        "\n",
        "  for epoch in range(max_iter):\n",
        "      running_loss = 0\n",
        "      for i, (images, labels) in enumerate(train_loader):\n",
        "          images = images.to(device)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          # Forward pass\n",
        "          outputs = model(images)\n",
        "          loss = criterion(outputs, labels)\n",
        "          \n",
        "          # Backward and optimize\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          curr_loss = loss.item()\n",
        "          if get_stats and ((i+1) % 100 == 0):\n",
        "              print (f'Epoch [{epoch+1}/{max_iter}], Step [{i+1}/{n_total_steps}], Loss: {curr_loss:.4f}')\n",
        "      \n",
        "      # checking convergence\n",
        "      if abs(prev_loss - curr_loss) < loss_tol :\n",
        "        if get_stats:\n",
        "          print(f'Completed in {epoch}')\n",
        "        return  \n",
        "      prev_loss = curr_loss\n",
        "  \n",
        "  # reached maximum iterations\n",
        "  if get_stats:    \n",
        "      print(f'Not converged yet, completed max {epoch} iterations')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKytezhEBKaZ"
      },
      "source": [
        "**Function to Test the model**\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL2O4txyBC46"
      },
      "source": [
        "def test(model, test_dataset, batch_size=5, model_desc=\"model\"):\n",
        "\n",
        "  # making batches of test data\n",
        "  batch_size = min(batch_size, len(test_dataset))\n",
        "  test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                            batch_size=batch_size, \n",
        "                                            shuffle=False)\n",
        "  \n",
        "  # We don't need to compute gradients \n",
        "  # (for memory efficiency) in the test phase\n",
        "  with torch.no_grad():\n",
        "      n_correct = 0\n",
        "      n_samples = 0\n",
        "      for images, labels in test_loader:\n",
        "          images = images.to(device)\n",
        "          labels = labels.to(device)\n",
        "          outputs = model(images)\n",
        "\n",
        "          # labels are the classes with maximum\n",
        "          # probability\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          n_samples += labels.size(0)\n",
        "          \n",
        "          # updating number of correct classification\n",
        "          # with matches of predicted and the actual labels\n",
        "          n_correct += (predicted == labels).sum().item()\n",
        "      \n",
        "      acc = 100.0 * n_correct / n_samples\n",
        "      print(f'Accuracy of the {model_desc} on {n_samples} test samples: {acc} %')\n",
        "      return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYMNDlv_Aib5"
      },
      "source": [
        "**Function to run train and test defined models**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTM51KaAvmEb"
      },
      "source": [
        "def run_models(inp_dim, out_dim, train_dataset, test_dataset, batch_size=5, max_iter=10, learn_rate=0.001, loss_tol=1e-3, criterion=nn.CrossEntropyLoss(), get_stats=False):\n",
        "  \n",
        "  ps_accs = []\n",
        "  \n",
        "  model_1 = nn.Linear(inp_dim, out_dim).to(device)\n",
        "  train(model_1, train_dataset, batch_size=batch_size, criterion=criterion, learn_rate=learn_rate, max_iter=max_iter, loss_tol=loss_tol, get_stats=get_stats)\n",
        "  ps_accs.append(test(model_1, test_dataset, batch_size=batch_size, model_desc=\"model 1\"))\n",
        "\n",
        "  model_2 = nn.Sequential(nn.Linear(inp_dim, 2), \n",
        "                          nn.ReLU(),\n",
        "                          nn.Linear(2, out_dim)).to(device)\n",
        "  train(model_2, train_dataset, batch_size=batch_size, criterion=criterion, learn_rate=learn_rate, max_iter=max_iter, loss_tol=loss_tol, get_stats=get_stats)\n",
        "  ps_accs.append(test(model_2, test_dataset, batch_size=batch_size, model_desc=\"model 2\"))\n",
        "\n",
        "  model_3 = nn.Sequential(nn.Linear(inp_dim, 6), \n",
        "                          nn.ReLU(),\n",
        "                          nn.Linear(6, out_dim)).to(device)\n",
        "  train(model_3, train_dataset, batch_size=batch_size, criterion=criterion, learn_rate=learn_rate, max_iter=max_iter, loss_tol=loss_tol, get_stats=get_stats)\n",
        "  ps_accs.append(test(model_3, test_dataset, batch_size=batch_size, model_desc=\"model 3\"))\n",
        "\n",
        "  model_4 = nn.Sequential(nn.Linear(inp_dim, 2), \n",
        "                          nn.ReLU(),\n",
        "                          nn.Linear(2, 3),\n",
        "                          nn.ReLU(),\n",
        "                          nn.Linear(3, out_dim)).to(device)\n",
        "  train(model_4, train_dataset, batch_size=batch_size, criterion=criterion, learn_rate=learn_rate, max_iter=max_iter, loss_tol=loss_tol, get_stats=get_stats)\n",
        "  ps_accs.append(test(model_4, test_dataset, batch_size=batch_size, model_desc=\"model 4\"))\n",
        "\n",
        "  model_5 = nn.Sequential(nn.Linear(inp_dim, 3), \n",
        "                          nn.ReLU(),\n",
        "                          nn.Linear(3, 2),\n",
        "                          nn.ReLU(),\n",
        "                          nn.Linear(2, out_dim)).to(device)\n",
        "  train(model_5, train_dataset, batch_size=batch_size, criterion=criterion, learn_rate=learn_rate, max_iter=max_iter, loss_tol=loss_tol, get_stats=get_stats)\n",
        "  ps_accs.append(test(model_5, test_dataset, batch_size=batch_size, model_desc=\"model 5\"))\n",
        "\n",
        "  return ps_accs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYKOO0RWqv6V"
      },
      "source": [
        "**Task 2**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBIlrZ9ePNF8",
        "outputId": "f1d52cc0-7826-43b9-edab-f08d9ab5acb6"
      },
      "source": [
        "# Hyperparameters\n",
        "inp_dim = 64\n",
        "out_dim = 10\n",
        "max_iter = 10\n",
        "batch_size = 5\n",
        "loss_tol=1e-3\n",
        "cont_lrs = [0.1, 0.01, 0.005, 0.001, 0.0001, 1e-5]\n",
        "\n",
        "# contains accuracies for all learning rate and models\n",
        "accs = []\n",
        "\n",
        "for learn_rate in cont_lrs:\n",
        "  print(f\"\\nFor learning rate = {learn_rate}\")\n",
        "  accs.append(run_models(inp_dim, out_dim, train_dataset, test_dataset, batch_size=batch_size, max_iter=max_iter, learn_rate=learn_rate, loss_tol=loss_tol, get_stats=False))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "For learning rate = 0.1\n",
            "Accuracy of the model 1 on 1797 test samples: 92.87701725097385 %\n",
            "Accuracy of the model 2 on 1797 test samples: 10.01669449081803 %\n",
            "Accuracy of the model 3 on 1797 test samples: 10.127991096271563 %\n",
            "Accuracy of the model 4 on 1797 test samples: 9.905397885364497 %\n",
            "Accuracy of the model 5 on 1797 test samples: 9.84974958263773 %\n",
            "\n",
            "For learning rate = 0.01\n",
            "Accuracy of the model 1 on 1797 test samples: 90.98497495826378 %\n",
            "Accuracy of the model 2 on 1797 test samples: 18.586533110740124 %\n",
            "Accuracy of the model 3 on 1797 test samples: 84.91930996104618 %\n",
            "Accuracy of the model 4 on 1797 test samples: 29.66054535336672 %\n",
            "Accuracy of the model 5 on 1797 test samples: 45.742904841402336 %\n",
            "\n",
            "For learning rate = 0.005\n",
            "Accuracy of the model 1 on 1797 test samples: 94.82470784641069 %\n",
            "Accuracy of the model 2 on 1797 test samples: 24.095715080690038 %\n",
            "Accuracy of the model 3 on 1797 test samples: 73.90094602114635 %\n",
            "Accuracy of the model 4 on 1797 test samples: 52.75459098497496 %\n",
            "Accuracy of the model 5 on 1797 test samples: 43.07178631051753 %\n",
            "\n",
            "For learning rate = 0.001\n",
            "Accuracy of the model 1 on 1797 test samples: 93.60044518642181 %\n",
            "Accuracy of the model 2 on 1797 test samples: 19.588202559821926 %\n",
            "Accuracy of the model 3 on 1797 test samples: 87.64607679465776 %\n",
            "Accuracy of the model 4 on 1797 test samples: 35.72621035058431 %\n",
            "Accuracy of the model 5 on 1797 test samples: 38.007790762381745 %\n",
            "\n",
            "For learning rate = 0.0001\n",
            "Accuracy of the model 1 on 1797 test samples: 90.70673344462993 %\n",
            "Accuracy of the model 2 on 1797 test samples: 24.819143016138007 %\n",
            "Accuracy of the model 3 on 1797 test samples: 49.58263772954925 %\n",
            "Accuracy of the model 4 on 1797 test samples: 16.360601001669448 %\n",
            "Accuracy of the model 5 on 1797 test samples: 19.69949916527546 %\n",
            "\n",
            "For learning rate = 1e-05\n",
            "Accuracy of the model 1 on 1797 test samples: 51.029493600445186 %\n",
            "Accuracy of the model 2 on 1797 test samples: 10.127991096271563 %\n",
            "Accuracy of the model 3 on 1797 test samples: 20.20033388981636 %\n",
            "Accuracy of the model 4 on 1797 test samples: 12.409571508069003 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmc-suWgqOpM"
      },
      "source": [
        "**Task 4**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPbOycrkX8qj"
      },
      "source": [
        "# plotting accuracy vs model for all learning rates\n",
        "plt.figure(figsize=(15, 9))\n",
        "for ind_lr in range(len(cont_lrs)):\n",
        "  plt.plot(range(1,len(accs[ind_lr])+1), accs[ind_lr], label=\"learning_rate=\"+str(cont_lrs[ind_lr]))\n",
        "plt.legend(loc = \"upper right\")\n",
        "plt.xlabel('model number')\n",
        "plt.ylabel('accuracy(%)')\n",
        "plt.xticks(range(1,len(accs[0])+1))\n",
        "# plt.grid(b=True)\n",
        "plt.title(\"model_vs_accuracy_for_learning_rate\")\n",
        "plt.savefig('model_vs_accuracy_for_learning_rate.png')\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "# plotting accuracy vs learning rates for all models\n",
        "plt.figure(figsize=(15, 9))\n",
        "for ind_mod in range(len(accs[0])):\n",
        "  plt.plot(np.log10(np.array(cont_lrs)), [accs[ind_lr][ind_mod] for ind_lr in range(len(cont_lrs))], label=\"model=\"+str(ind_mod+1))\n",
        "plt.legend(loc = \"upper left\")\n",
        "plt.xlabel('log10(learning rate)')\n",
        "plt.ylabel('accuracy(%)')\n",
        "plt.xticks(np.log10(np.array(cont_lrs)))\n",
        "# plt.grid(b=True)\n",
        "plt.title(\"learning_rate_vs_accuracy_for_model\")\n",
        "plt.savefig('learning_rate_vs_accuracy_for_model.png')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fabOGVV4fxgv"
      },
      "source": [
        "# **PCA** **Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "od_c1sZHpheb"
      },
      "source": [
        "**Task 5**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4WftDTCcGfz"
      },
      "source": [
        "# Reducing train and test feature space\n",
        "train_feat_tensor = torch.as_tensor([tup[0].tolist() for tup in train_dataset])\n",
        "U,S,V = torch.pca_lowrank(train_feat_tensor, q=None, center=True, niter=3)\n",
        "red_train_feat = torch.matmul(train_feat_tensor, V[:, :2]).tolist()\n",
        "red_train_data = [(torch.tensor(red_train_feat[i]), train_dataset[i][1]) for i in range(len(train_dataset))]\n",
        "\n",
        "# using V to reduce the test feature tensors\n",
        "test_feat_tensor = torch.as_tensor([tup[0].tolist() for tup in test_dataset])\n",
        "red_test_feat = torch.matmul(test_feat_tensor, V[:, :2]).tolist()\n",
        "red_test_data = [(torch.tensor(red_test_feat[i]), test_dataset[i][1]) for i in range(len(test_dataset))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jITq6caFYlRf"
      },
      "source": [
        "# plotting reduced train features\n",
        "plt.figure(figsize=(24, 24))\n",
        "label_color = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n",
        "color_tak = [0]*10\n",
        "\n",
        "for p_ind in range(len(train_dataset)):\n",
        "  curr_label = train_dataset[p_ind][1]\n",
        "  if color_tak[curr_label]:\n",
        "    plt.plot(red_train_feat[p_ind][0], red_train_feat[p_ind][1], marker=\"o\", markersize=5, markeredgecolor='none', markerfacecolor=label_color[curr_label])\n",
        "  else:\n",
        "    plt.plot(red_train_feat[p_ind][0], red_train_feat[p_ind][1], marker=\"o\", markersize=5, markeredgecolor='none',markerfacecolor=label_color[curr_label], label=\"digit \"+str(curr_label))\n",
        "  color_tak[curr_label] = 1\n",
        "\n",
        "plt.legend(loc=\"upper left\", fontsize=20, ncol = 2))\n",
        "plt.savefig('pca_on_training_data_visual.png')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Q4vUfzep0m6"
      },
      "source": [
        "**Task 6**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuaZttetVks4"
      },
      "source": [
        "cont_lrs = [0.001]\n",
        "inp_dim = 2\n",
        "out_dim = 10\n",
        "max_iter = 10\n",
        "batch_size = 5\n",
        "loss_tol=1e-3\n",
        "accs = []\n",
        "for learn_rate in cont_lrs:\n",
        "  print(f\"\\nFor learning rate = {learn_rate}\")\n",
        "  accs.append(run_models(inp_dim, out_dim, red_train_data, red_test_data, batch_size=batch_size, max_iter=max_iter, learn_rate=learn_rate, loss_tol=loss_tol, get_stats=False))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}